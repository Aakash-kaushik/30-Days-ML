{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "### By **[NimbleBox](https://www.nimblebox.ai)**\n",
    "\n",
    "\n",
    "[<img src=\"./assets/nbx.jpeg\" alt=\"NimbleBox.ai logo\" width=\"600\"/>](https://www.nimblebox.ai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we do\n",
    " \n",
    "In this notebook we are going to implement a neural network from scratch in python and this time we are going to build a different neural network then the one that we used to explain neural networks. This time we are going to solve a classification problem. So there will be some minor changes to the network to use it for Classification. Let's look at them.\n",
    " \n",
    "## Softmax \n",
    " \n",
    "A softmax function takes a vector of input and gives out a vector with probabilities that obviously add up to one because they are probabilities and in our case a softmax unit at the end will tell us the probability of the class that the neural network thinks it is.\n",
    " \n",
    "Softmax is calculated by taking the elementwise exponent of the vector and then dividing elementwise by the sum of the vector after taking the exponent.\n",
    " \n",
    "<img src=\"./assets/softmax.png\" width=150>\n",
    " \n",
    " \n",
    "so let's take an example in our implementation we will be using the iris dataset which has 3 flowers['setosa', 'versicolor', 'virginica'] in the same order so for one example the true output looks like this.\n",
    " \n",
    "$$ Y = [0, 1, 0] $$\n",
    " \n",
    "This means that the second node in our neural network denotes a particular flower which is versicolor and we will be expecting a softmax output something like this.\n",
    " \n",
    "$$ Y_hat = [0.3, 0.95, 0.2] $$\n",
    " \n",
    "We pick the largest number in the vector to decide which class the neural network thinks this example denotes to.\n",
    " \n",
    "## Loss Function\n",
    " \n",
    "We will also use a new loss function named **Cross Entropy loss** to measure the classification loss between multiple classes.\n",
    " \n",
    "the formula for the loss is this where $ y_o $ is Y or the labels and $ P_o $ is the predicted label which you will also see in the below implementation. \n",
    " \n",
    "<img src=\"./assets/cross_entropy.png\" width=150>\n",
    " \n",
    " \n",
    "we will change the input units or the inputs($ X $) to 4 and the number of hidden units to 5 and the output is going to be 3 units as there are 3 flowers Now let's see the architecture of the neural network.\n",
    " \n",
    "<img src=\"./assets/nn.png\" width=700>\n",
    " \n",
    "The Forward, Backward and Gradient steps are going to remain the same, the only difference is that now instead of a scalar they are going to be matrices. Let's have a look over these steps again and I will be mentioning the matrix dimensions in front of all the variables this time.\n",
    " \n",
    "### Forward Propagation step\n",
    " \n",
    "As we know that we will be using a sigmoid function instead of $ g() $ and a softmax function at the end I will also be replacing that.\n",
    " \n",
    "1. $ Z_1 = W_1[5,4] * X[4,150] + b_1[5, 1]$\n",
    "2. $ A_1 = sigmoid(Z)[5, 150] $\n",
    "3. $ Z_2 = softmax(W_2[3, 5] * A_1[5,150] + b_2[3 ,1])$\n",
    "4. $ Y\\_hat = Z_2[3, 150] $\n",
    " \n",
    "### Backward Propagation step\n",
    " \n",
    "For backward Propagation we only used $ W1 $ as our example and the change their is that instead of calculating the derivative for a scalar we will be calculating the derivative for the whole vector or matrix but element wise. Let's take an example and suppose we had a vector A like the one below. \n",
    " \n",
    "$$ A = [2, 4, 5] $$\n",
    " \n",
    "So if say do an element wise square on $ A $, The result will be.\n",
    " \n",
    "$$ A^2 = [4, 16, 25] $$\n",
    " \n",
    "And as the loss function is changed the derivative of $ ∂E/∂Y\\_hat $ is also changed.\n",
    " \n",
    "1. $ ∂E/∂Y\\_hat = Y_hat - Y $\n",
    "2. $ ∂Y\\_hat/∂A = W_2 $\n",
    "3. $ ∂A/∂Z = A*(1 - A) $\n",
    "4. $ ∂Z/∂W_1 = X $\n",
    " \n",
    "finally the whole derivative for $ ∂E/∂W_1 $ will be.\n",
    " \n",
    "$$ ∂E/∂W_1 = (Y_hat - Y) * W_2 * A*(1 - A) * X $$\n",
    " \n",
    "### Gradient descent\n",
    " \n",
    "$$ W_1[5, 4] = W_1[5, 4] - α[1, 1] * ∂E/∂W_1[5, 4] $$ \n",
    "$$ b_1[5, 1] = b_1[5, 1] - α[1, 1] * ∂E/∂b_1[5, 1] $$\n",
    "$$ W_2[3, 5] = W_2[3, 5] - α[1, 1] * ∂E/∂W_2[3, 5] $$\n",
    "$$ b_2[3, 1] = b_2[3, 1] - α[1, 1] * ∂E/∂b_2[3, 1] $$\n",
    " \n",
    "Where $ α $ is going to be a scalar which we will broadcast to match the shape of our matrix to which it subtracts with. \n",
    " \n",
    "## Implementation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "def sigmoid(X):\n",
    "  return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "  return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(X):\n",
    "  e = np.exp(X - X.max())\n",
    "  return (e / e.sum(axis=1, keepdims=True))\n",
    "\n",
    "def loss(Y, Y_hat):\n",
    "  loss = Y * np.log(Y_hat)\n",
    "  return -np.sum(loss)\n",
    "\n",
    "def neural_network_train(X, Y, num_iteration=1200):\n",
    "  # Random initializing the weights and bias\n",
    "\n",
    "  W_1 = np.random.randn(5, 4)\n",
    "  b_1 = np.random.randn(1, 5)\n",
    "  \n",
    "  W_2 = np.random.randn(3, 5)\n",
    "  b_2 = np.random.randn(1, 3)\n",
    "\n",
    "  # Defining the learning rate\n",
    "\n",
    "  lr = 1e-3\n",
    "\n",
    "  for iteration in range(num_iteration):\n",
    "      # Forward Propagation\n",
    "\n",
    "    Z_1 = np.dot(X, W_1.T) + b_1\n",
    "    A_1 = sigmoid(Z_1)\n",
    "    Z_2 = np.dot(A_1, W_2.T) + b_2\n",
    "    Y_hat = softmax(Z_2)\n",
    "\n",
    "    # Backward Propagation\n",
    "\n",
    "    dE_dY_hat = Y_hat - Y\n",
    "    dY_hat_dW_2 = A_1\n",
    "\n",
    "    dE_dW_2 = np.dot(dY_hat_dW_2.T, dE_dY_hat)\n",
    "\n",
    "    dE_db_2 = dE_dY_hat\n",
    "\n",
    "    dZ_2_dA_1 = W_2\n",
    "    dE_dA_1 = np.dot(dE_dY_hat, dZ_2_dA_1)\n",
    "    dA_1_dZ_1 = sigmoid_der(Z_1)\n",
    "    dZ_1_dW_1 = X\n",
    "    dE_dW_1 = np.dot(dZ_1_dW_1.T, dA_1_dZ_1 * dE_dA_1)\n",
    "\n",
    "    dE_db_1 = dE_dA_1 * dA_1_dZ_1\n",
    "\n",
    "    # Gradient Descent\n",
    "\n",
    "    W_2 = W_2 - lr * dE_dW_2.T\n",
    "    b_2 = b_2 - lr * dE_db_2.sum(axis=0)\n",
    "\n",
    "    W_1 = W_1 - lr * dE_dW_1.T\n",
    "    b_1 = b_1 - lr * dE_db_1.sum(axis=0)\n",
    "\n",
    "\n",
    "    if iteration % 5 == 0:\n",
    "      print(Y.shape, Y_hat.shape)\n",
    "      print(\"iteration : \", iteration, \"   loss : \", loss(Y, Y_hat))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  X, Y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "  # Preprocessing the data to have a mean 0 and variance 1\n",
    "  X = preprocessing.scale(X)\n",
    "  # Y has the shape (150, ) rather than (150, 1)\n",
    "  Y = Y.reshape(150,1)\n",
    "  neural_network_train(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do next\n",
    " \n",
    "As you can see that we haven't chosen the best parameters. There are a lot of things that you can tune. let's mention some of them.\n",
    " \n",
    "- Learning rate.\n",
    "- Number of hidden units.\n",
    "- Number of hidden layers.\n",
    "- weight initialization method.\n",
    "- activation function used: You can use ReLU or any other activation function instead of sigmoid.\n",
    "- number of iterations.\n",
    "- Type of classification loss.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
