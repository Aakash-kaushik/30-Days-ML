{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "### By **[NimbleBox](https://www.nimblebox.ai)**\n",
    "\n",
    "[<img src=\"./assets/nbx.jpeg\" alt=\"NimbleBox.ai logo\" width=\"600\"/>](https://www.nimblebox.ai)\n",
    "\n",
    "Parts of this notebook are from [Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we learn \n",
    "\n",
    "In this Notebook we are going to learn the difference about parameters and hyperparameters, which hyperparameters has a bigger impact on improving the performance and accuracy then we are going to talk about some approaches to find hyperparameters and then we are going to talk about two approaches on how to tune your model and find the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters VS Hyperparameters\n",
    " \n",
    "Let's talk about their differences, So we have talked about weight and biases and these are some of the things that your model uses to predict the output given a certain input, These are called parameters whereas hyperparameters are the ones that you use to train your model and in turn modify parameters some examples of hyperparameters are learning rate, type of optimization algorithm, number of iterations in the optimization algorithm ,activation function, number of layers in a neural network about which we are going to study in the coming weeks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Hyperparameters\n",
    " \n",
    "There are so many hyperparameters to tune and which should you tune first, So there is no hard rule that says that you have to tune so and so parameters first but as you practice machine learning you get an idea that certain hyperparameters have more impact on accuracy or the model performance than others and it's better to focus your efforts on that, but just to get you started we will list down some of them in descending order of their priority that you can try.\n",
    " \n",
    "- Learning rate.\n",
    "- Parameters of your optimization algorithm.\n",
    "- Mini batch size.\n",
    "- Hidden number of units in neural networks.\n",
    "- Hidden layers in a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching \n",
    " \n",
    "So now you know what are hyperparameters and which ones are better to tune first or give more importance to, With that let's talk about how you are going to search them, so for this we are going to assume that for now we are only trying to search for two hyperparameters which can be anything from the ones mentioned above or something else too.\n",
    " \n",
    "### Grid Search \n",
    " \n",
    "So as the name says Grid search is a type of search in which you define the scale or the end points of the hyperparameters that you want to search and then you train and evaluate your model on every value on that grid to find a better pair of hyperparameters as there were only two in our case. Let's look at a picture to get a fair idea. \n",
    " \n",
    "<img src=\"./assets/grid_search.png\" width=700>\n",
    " \n",
    "### Random Grid Search\n",
    " \n",
    "Random grid search is similar to grid search except we now sample hyperparameters points to try at random. But why would we want to do such a thing because when we have grid search with which we explore every point and can find the best ones. Grid search even with just two parameters and a small scale to search is still very computationally expensive and in the era of deep learning where we don't just have two parameters but a lot of them on a larger scale it's not feasible with respect to computing power and the time that it will take to try every point out there. That's where random grid search comes in and practically it's very effective in finding a good pair of hyperparameters and mostly far more quickly and with less resources than grid search.\n",
    " \n",
    "<img src=\"./assets/random_grid_search.png\" width=700>\n",
    " \n",
    "As you can see in the image that the points are randomly sampled.\n",
    "\n",
    "So you can use grid search when the number of hyperparameters to tune is less and you should prefer random grid search when they are just too much.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the best\n",
    " \n",
    "You can find good hyperparameters with grid search and random grid search but to get the best hyperparameters you do just one more step, When you see that you are getting good results with your model in a certain are of the grid you zoom in, what we mean by that is you just ignore all the other parts of the grid and sample more finer points in that area which can also be said as searching from a coarse to fine scale and you can repeat this process or you can again try a searching method and try a different location this time.\n",
    " \n",
    "<img src=\"./assets/coarse_to_fine.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Approaches \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
